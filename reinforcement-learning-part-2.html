<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <title>Flaport.net | Reinforcement learning from the ground up | part 2: deep Q learning on tic-tac-toe.</title>
    
    <link rel="shortcut icon" type="image/png" href="" />
    <link rel="stylesheet" href="/static/css/normalize.css" type="text/css" />
    <link rel="stylesheet" href="/static/css/fonts.css" type="text/css" />
    <link rel="stylesheet" href="/static/css/main.css" type="text/css" />
    <link rel="stylesheet" href="/static/css/colors.css" type="text/css" />
    
    
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
        },
      };
    </script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <script
      crossorigin="anonymous"
      integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA="
      src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"
    ></script>
   
  </head>
  <body>
    <div class="document">
      <div class="contentwrapper">
          <div class="fa noprint" align="right">
            ï††&nbsp;<input type="checkbox" class="noprint" id="darkmodetoggle" />
          </div>
        <div class="content">
            
    <h1>Reinforcement learning from the ground up | part 2: deep Q learning on tic-tac-toe.</h1>
<!-- author: Flaport&nbsp;&middot; -->
posted on <a href="#" class="date">2019-12-22T14:53:34Z</a>&nbsp;&middot;

last modified on <a href="#" class="date">2020-05-30T11:27:03Z</a>&nbsp;&middot;


tags:


    
        <span><a href="/tags/machine-learning.html">machine-learning</a>  </span>
    

    
        <span><a href="/tags/python.html">python</a>  </span>
    

    
        <span><a href="/tags/reinforcement-learning.html">reinforcement-learning</a>  </span>
    

    <div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Previously, we saw that reinforcement learning worked quite well on tic-tac-toe. However, there's something unsatisfying about working with a Q-table storing all the possible states of the game. It feels like the Agent simply memorizes each state of the game and acts according to some memorized rules obtained by its huge amount of experience (remember that the Agent played 10,000,000 games during training). In this second part of the reinforcement learning series, we'll swap out the Q table for a <em>neural network</em>.</p>
<p><img src="static/img/reinforcement-learning/tictactoe.svg" width=200></p>
<p><strong>The AI created in this series can now be challenged <a href="/reinforcement-learning-part-3.html">here</a>!</strong></p>
<p>We'll use PyTorch to create and optimize the neural network, as its excellent compatibility with numpy allows us to swap out the Qtable for a deep Q network with minimal effort. Also, it's just the best deep-learning framework for python - no competition really.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Index">Index<a class="anchor-link" href="#Index">&#182;</a></h2><ul>
<li><a href="/reinforcement-learning-part-1.html">part 1</a>: We create the game environment and a simple unbeatable AI based on <em>traditional</em> Q-learning ðŸ¤–.</li>
<li>part 2 <strong>(this post)</strong>: We modify our AI to utilize a neural network: <em>deep</em> Q-learning ðŸ‘¾.</li>
<li><a href="/reinforcement-learning-part-3.html">part 3</a>: Have some fun and play against the Q-agent ðŸ¤“.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#!</span>
<span class="c1"># standard library:</span>
<span class="kn">import</span> <span class="nn">json</span>  <span class="c1"># to store learned state</span>

<span class="c1"># 3rd party:</span>
<span class="kn">import</span> <span class="nn">tqdm</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># settings</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Q-model">Q-model<a class="anchor-link" href="#Q-model">&#182;</a></h2><p>The main difference with the <a href="/20190527_reinforcement-learning-on-tictactoe-part-1.html">previous part</a> of the series is that we'll swap out the Q-table by a <em>deep Q neural network model</em> of 3 layers deep. OK, that's not very deep, but we'll stick with the buzzwords.</p>
<p>The goal of our <code>QModel</code> is to accurately <em>approximate</em> the values in the <code>QTable</code>. We could thus train the neural network on the values obtained in the previous part, but obviously that would defeat the purpose of using a neural network in the first place (as it requires you to find the underlying Qtable first).</p>
<p>We want the neural network to act as a <em>proxy</em> for the table. It also has to be able to approximate the QTable in cases it is completely unfeasible to find the underlying QTable.</p>
<p>The <code>QModel</code> defined below expects the full game state (2D game state and the turn). The 2D state will be flattened and concatenated with the turn index. This concatenated state of length 10 with three kinds of states per cell (<code>1</code>, <code>2</code> or <code>0</code>) is then embedded in a three dimensional embedding space. Next, the embedded game state is sent through 3 neural network layers with ReLU activation.</p>
<p>Note that in this case, the turn index <em>is</em> used to make a prediction. This is in contrast with the Q-table case, where just the 2D state was used. The reason for this is that in this case we work with a neural network for which the predicted Q-values are note completely independent for each state given. We thus want to give as much information to the network as possible and let the network itself figure out which information it uses.</p>
<p>Apart from the network definition, we also created some <code>save</code> and <code>load</code> methods to save the weights of the network as <code>json</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">QModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states2d</span><span class="p">,</span> <span class="n">turns</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">states2d</span><span class="p">):</span>
            <span class="n">states2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">states2d</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">turns</span><span class="p">):</span>
            <span class="n">turns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">turns</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">states2d</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="c1"># batch dimension required</span>
        <span class="k">assert</span> <span class="n">turns</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="c1"># only dim = batch dim</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">states2d</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">turns</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_serialize_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_serialize_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_deserialize_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.json&quot;</span><span class="p">):</span>
            <span class="n">filename</span> <span class="o">+=</span> <span class="s2">&quot;.json&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span>
                <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_serialize_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
                <span class="n">file</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.json&quot;</span><span class="p">):</span>
            <span class="n">filename</span> <span class="o">+=</span> <span class="s2">&quot;.json&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_deserialize_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Game">The Game<a class="anchor-link" href="#The-Game">&#182;</a></h2><p>In the <a href="/reinforcement-learning-part-1.html">previous part</a>, we assumed the Agent learned after every turn played. This form of <em>online</em> learning worked well for a Q-table, as each state has an independent row of qvalues associated with it.</p>
<p>Unfortunaty, when using a neural network, predicted Q values will not be completely independent for each different state. This is a good thing, as it allows the neural network to generalize and invoke similar behavior for similar states.</p>
<p>However, it also makes the training process less stable. To be able to stabilize the training, we'll have to batch several <em>transitions</em> - (<code>state</code>, <code>action</code>, <code>next_state</code>, <code>reward</code>) sequences - between states together.</p>
<p>Then, after a few games are played, all transitions are batched together and the Agent can learn from each transition in the batch simultaneously.</p>
<p>Let's see how we can implement this:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">TicTacToe</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Tic-Tac-Toe Game &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">player1</span><span class="p">,</span> <span class="n">player2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; The Tic-Tac-Toe game takes two players and pitches them against each other. &quot;&quot;&quot;</span>
        <span class="c1"># pitch players against each other</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">players</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="n">player1</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="n">player2</span><span class="p">}</span>

        <span class="c1"># reward for each outcome of the game (tie, player1 wins, player2 wins)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reward</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_games</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; play several full games &quot;&quot;&quot;</span>

        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_games</span><span class="p">):</span>
            <span class="n">turn</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">state2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state2d</span><span class="p">,</span> <span class="n">turn</span><span class="p">)</span> <span class="c1"># full state of the game</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
                <span class="n">current_player</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">players</span><span class="p">[</span><span class="n">turn</span><span class="p">]</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">current_player</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">play_turn</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
                <span class="n">transitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">visualize</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">visualize_state</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">turn</span><span class="p">)</span>
                    
                <span class="p">(</span><span class="n">state2d</span><span class="p">,</span> <span class="n">turn</span><span class="p">)</span> <span class="o">=</span> <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
                
                <span class="k">if</span> <span class="n">turn</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">break</span>

        <span class="k">return</span> <span class="n">transitions</span>
    
    <span class="k">def</span> <span class="nf">play_turn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; execute a specific move chosen by the current player and </span>
<span class="sd">        check if it&#39;s a winning/losing move. &quot;&quot;&quot;</span>
        <span class="c1"># retrieve states</span>
        <span class="n">state2d</span><span class="p">,</span> <span class="n">turn</span> <span class="o">=</span> <span class="n">state</span>
        <span class="n">next_state2d</span> <span class="o">=</span> <span class="n">state2d</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">next_turn</span> <span class="o">=</span> <span class="n">turn</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># transform action in two indices</span>
        <span class="n">ax</span><span class="p">,</span> <span class="n">ay</span> <span class="o">=</span> <span class="n">action</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">action</span> <span class="o">%</span> <span class="mi">3</span>

        <span class="c1"># check if board is already occupied at location</span>
        <span class="k">if</span> <span class="n">state2d</span><span class="p">[</span><span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># invalid move</span>
            <span class="n">next_state2d</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state2d</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># next_turn == 0 -&gt; game over</span>
            <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward</span><span class="p">[</span><span class="n">next_turn</span><span class="p">]</span>  <span class="c1"># next player wins</span>

        <span class="c1"># apply action</span>
        <span class="n">next_state2d</span><span class="p">[</span><span class="n">ax</span><span class="p">,</span> <span class="n">ay</span><span class="p">]</span> <span class="o">=</span> <span class="n">turn</span>

        <span class="c1"># check if the action resulted in a winner</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">next_state2d</span> <span class="o">==</span> <span class="n">turn</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
            <span class="ow">or</span> <span class="p">(</span><span class="n">mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
            <span class="ow">or</span> <span class="p">(</span><span class="n">mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
            <span class="ow">or</span> <span class="p">(</span><span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
            <span class="ow">or</span> <span class="p">(</span><span class="n">mask</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
            <span class="ow">or</span> <span class="p">(</span><span class="n">mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
            <span class="ow">or</span> <span class="p">(</span><span class="n">mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="ow">or</span> <span class="p">(</span><span class="n">mask</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="ow">and</span> <span class="n">mask</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="p">):</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state2d</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># next_turn == 0 -&gt; game over</span>
            <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward</span><span class="p">[</span><span class="n">turn</span><span class="p">]</span>  <span class="c1"># current player wins</span>

        <span class="c1"># if the playing board is full, but no winner found: tie</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">next_state2d</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>  <span class="c1"># final tie.</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state2d</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># next_turn == 0 -&gt; game over</span>
            <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># no winner</span>

        <span class="c1"># if no move has resulted in a winner: next player&#39;s turn.</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state2d</span><span class="p">,</span> <span class="n">next_turn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># no winner yet</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">visualize_state</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">turn</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; show the resulting game state after a player&#39;s turn &quot;&quot;&quot;</span>
        <span class="n">next_state2d</span><span class="p">,</span> <span class="n">next_turn</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;player </span><span class="si">{</span><span class="n">turn</span><span class="si">}</span><span class="s2">&#39;s turn:&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">next_state2d</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="ow">and</span> <span class="n">turn</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[invalid state]</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="nb">str</span><span class="p">(</span><span class="n">next_state2d</span><span class="p">)</span>
                <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;[[&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; [&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;]]&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;]&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
                <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">)</span>
                <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;X&quot;</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Deep-QAgent">Deep QAgent<a class="anchor-link" href="#Deep-QAgent">&#182;</a></h2><p><img src="static/img/reinforcement-learning/robot.png" width=200></p>
<p>Finally, we have to adapt the Agent to</p>
<ol>
<li>Use the <code>QModel</code> in stead of the <code>QTable</code></li>
<li>Learn from multiple transitions at once instead of a single one.</li>
</ol>
<p>One of the main observations used to be able to learn from a batch of transitions at once is the observation that:</p>
\begin{align*}
    \rm argmin(\texttt{values}) = argmax(-\texttt{values})
\end{align*}<p>and</p>
\begin{align*}
    \rm min(\texttt{values}) = -max(-\texttt{values})
\end{align*}<p>This allows us to unify the policies for player1 and player2 in a single policy which depends on the parity <code>s</code> of your turn (<code>+1</code> for player 1, <code>-1</code> for player 2):</p>
\begin{align*}
    \texttt{action} &amp;= \pi(\texttt{state}) = \text{argmax}_{action} \left( s \cdot Q(\texttt{state},~\texttt{action}) \right)
\end{align*}<p>This same trick can also be used to calculate the error Î´ between the expected qvalues and the discounted future qvalues:
\begin{align*}
    \delta &amp;= Q(\texttt{state},~\texttt{action}) - (\texttt{reward} + s \cdot \texttt{discount_factor} \cdot \text{max}_{\texttt{action}} \left( s \cdot Q(\texttt{next-state},~\texttt{action})) \right)
\end{align*}</p>
<p>Unfortunately, this error can not be used explicitly anymore for updating the qtable. In stead, we use PyTorch's Adam optimizer to minimize the <em>Huber Loss</em>, which is defined as:</p>
\begin{align*}
\begin{split}\quad \mathcal{L}(\delta) = \begin{cases}
  \frac{1}{2}{\delta^2}  &amp; \text{for } |\delta| \le 1, \\
  |\delta| - \frac{1}{2} &amp; \text{otherwise.}
\end{cases}\end{split}
\end{align*}<p>The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large. This makes it more robust to outliers when the estimates of Q are very noisy - which is very often the case with reinforcement learning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; The Agent plays the game by playing a move corresponding to the optimal Q-value &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">qmodel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">discount_factor</span><span class="o">=</span><span class="mf">0.9</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; A new Agent can be given some optional parameters to tune how fast it learns</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            qmodel: QModel=None: the initial Q-table to start with. </span>
<span class="sd">            epsilon: float=0.2: the chance the Agent will explore a random move</span>
<span class="sd">                               (in stead of choosing the optimal choice according to the Q table)</span>
<span class="sd">            learning_rate: float=0.3: the rate at which the Agent learns from its games</span>
<span class="sd">            discount_factor: float=0.9: the rate at which the final reward gets discounted</span>
<span class="sd">                                        for when rating previous moves.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qmodel</span> <span class="o">=</span> <span class="n">QModel</span><span class="p">()</span> <span class="k">if</span> <span class="n">qmodel</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">qmodel</span>

        <span class="c1"># the speed at which the Qvalues get updated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="c1"># pytorch optimizer to update the weights of the QModel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qmodel</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="c1"># the discount factor of future rewards</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="n">discount_factor</span>

        <span class="c1"># the chance of executing a random action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>

    <span class="k">def</span> <span class="nf">random_action</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; get a random action chosen from the allowed actions to take &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">best_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; get the best values according to the current Q table &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">state2d</span><span class="p">,</span> <span class="n">turn</span> <span class="o">=</span> <span class="n">state</span>
            <span class="n">sign</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">turn</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">turns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">turn</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)[</span><span class="kc">None</span><span class="p">]</span>  <span class="c1"># batch dim required</span>
            <span class="n">states2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">state2d</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)[</span><span class="kc">None</span><span class="p">]</span>
            <span class="n">qvalues</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qmodel</span><span class="p">(</span><span class="n">states2d</span><span class="p">,</span> <span class="n">turns</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">sign</span> <span class="o">*</span> <span class="n">qvalues</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; perform an action according to the state on the game board &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="c1"># Choose action (random with chance of epsilon; best action otherwise.)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_action</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># get qvalues for current state of the game</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transitions</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; learn from the current state and action taken. &quot;&quot;&quot;</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">rewards</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">transitions</span><span class="p">)</span>
        <span class="n">states2d</span><span class="p">,</span> <span class="n">turns</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">states</span><span class="p">)</span>
        <span class="n">next_states2d</span><span class="p">,</span> <span class="n">next_turns</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">next_states</span><span class="p">)</span>
        <span class="n">turns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">turns</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">next_turns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">next_turns</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">states2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">states2d</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">next_states2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">next_states2d</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># get qvalues for current state of the game</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_turns</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># wether the game is over or not</span>
            <span class="n">signs</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">next_turns</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">next_qvalues</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qmodel</span><span class="p">(</span><span class="n">next_states2d</span><span class="p">,</span> <span class="n">next_turns</span><span class="p">)</span>
            <span class="n">expected_qvalues_for_actions</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">signs</span> <span class="o">*</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">signs</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">next_qvalues</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># update qvalues:</span>
        <span class="n">qvalues_for_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qmodel</span><span class="p">(</span><span class="n">states2d</span><span class="p">,</span> <span class="n">turns</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">actions</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span>
            <span class="n">qvalues_for_actions</span><span class="p">,</span> <span class="n">expected_qvalues_for_actions</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train">Train<a class="anchor-link" href="#Train">&#182;</a></h2><p><img src="static/img/reinforcement-learning/train.jpg" width=300></p>
<p>We let the agent now train in batches of 1,000 games, for 15,000 epochs.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># initialize</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">total_number_of_games</span> <span class="o">=</span> <span class="mi">15_000_000</span> <span class="c1"># 15_000_000</span>
<span class="n">number_of_games_per_batch</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">player</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># 0.7 0.01</span>
<span class="n">game</span> <span class="o">=</span> <span class="n">TicTacToe</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="n">player</span><span class="p">)</span>

<span class="n">min_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
<span class="n">range_</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">trange</span><span class="p">(</span><span class="n">total_number_of_games</span> <span class="o">//</span> <span class="n">number_of_games_per_batch</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">range_</span><span class="p">:</span>
    <span class="n">transitions</span> <span class="o">=</span> <span class="n">game</span><span class="o">.</span><span class="n">play</span><span class="p">(</span><span class="n">num_games</span><span class="o">=</span><span class="n">number_of_games_per_batch</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">transitions</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">player</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">transitions</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">min_loss</span> <span class="ow">and</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="mf">0.01</span><span class="p">:</span>
        <span class="n">min_loss</span> <span class="o">=</span> <span class="n">loss</span>

    <span class="n">range_</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">min_loss</span><span class="o">=</span><span class="n">min_loss</span><span class="p">)</span>

<span class="n">player</span><span class="o">.</span><span class="n">qmodel</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;qmodel.json&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [2:18:48&lt;00:00,  1.81it/s, loss=0.00184, min_loss=0.001]   
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Let's-play-another-game!">Let's play another game!<a class="anchor-link" href="#Let's-play-another-game!">&#182;</a></h2><p>Let the trained agent play against itself one final time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">player</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># epsilon=0 -&gt; no random guesses</span>
<span class="n">game</span> <span class="o">=</span> <span class="n">TicTacToe</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="n">player</span><span class="p">)</span>
<span class="n">player</span><span class="o">.</span><span class="n">qmodel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;qmodel.json&quot;</span><span class="p">)</span>

<span class="c1"># play</span>
<span class="n">game</span><span class="o">.</span><span class="n">play</span><span class="p">(</span><span class="n">num_games</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>player 1&#39;s turn:
. . .
. O .
. . .


player 2&#39;s turn:
X . .
. O .
. . .


player 1&#39;s turn:
X . .
O O .
. . .


player 2&#39;s turn:
X . .
O O X
. . .


player 1&#39;s turn:
X . .
O O X
. O .


player 2&#39;s turn:
X X .
O O X
. O .


player 1&#39;s turn:
X X O
O O X
. O .


player 2&#39;s turn:
X X O
O O X
X O .


player 1&#39;s turn:
X X O
O O X
X O O


</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We see that the game also ends in a tie, providing some evidence that the Agent is as good as the one we made in the previous post.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Q-model for the trained Agent can be downloaded in json format <a href="https://data.flaport.net/qmodel.json">here</a> (availability not guarenteed).</p>

</div>
</div>
</div>
 



<script data-isso="//isso.flaport.net/" src="//isso.flaport.net/js/embed.min.js"></script>
<style>
#isso-thread > h4 {
    color: var(--color15);
    font-weight: bold;
}
</style>
<section id="isso-thread"></section>


        </div>
      </div>

      <div class="sidebar">
        <div class="sidebarwrapper">
          <h1 class="logo"><a href="/">Flaport.net</a></h1>

          <h3>Blog</h3>
          <ul>
            <li class="toctree-l1">
              <a href="/">Home</a>
            </li>
            
            
            <li class="toctree-l1">
                <a href="/configuring-neomutt-for-email.html">Next&nbsp;(newer)</a>
            </li>
            
            
            <li class="toctree-l1">
                <a href="/reinforcement-learning-part-1.html">Previous&nbsp;(older)</a>
            </li>
            
            <li class="toctree-l1">
                <a href="/index.xml" target="_blank">RSS&nbsp;<span class="fa">ï…ƒ</span></a>
            </li>
            <h3 style="padding-top: 1em;">Tags</h3>
            
            <li class="toctree-l1">
                <a href="/tags/machine-learning.html">machine-learning</a> (4)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/python.html">python</a> (3)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/reinforcement-learning.html">reinforcement-learning</a> (3)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/linux.html">linux</a> (2)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/computer-vision.html">computer-vision</a> (1)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/game.html">game</a> (1)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/javascript.html">javascript</a> (1)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/productivity.html">productivity</a> (1)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/tui.html">tui</a> (1)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/vps.html">vps</a> (1)
            </li>
            
            <li class="toctree-l1">
                <a href="/tags/yolo.html">yolo</a> (1)
            </li>
            
            <h3 style="padding-top: 1em;">Projects</h3>
            <li class="toctree-l1">
              <a href="https://photontorch.com"
                >Photontorch</a
              >
            </li>
            <li class="toctree-l1">
              <a href="https://github.com/flaport/fdtd"
                >Python&nbsp;3D&nbsp;FDTD</a
              >
            </li>
            <h3 style="padding-top: 1em;">External links</h3>
            <li class="toctree-l1">
              <a href="https://github.com/flaport"
                >GitHub&nbsp;<span class="fa">ï‚’&nbsp;</span></a
              >
            </li>
            <li class="toctree-l1">
              <a href="https://twitter.com/florislaporte"
                >Twitter&nbsp;<span class="fa">ï‚™&nbsp;</span></a
              >
            </li>
            <li class="toctree-l1">
              <a href="https://linkedin.com/in/florislaporte"
                >Linked&nbsp;<span class="fa">ïƒ¡&nbsp;</span></a
              >
            </li>
            <li class="toctree-l1">
              <a href="https://www.photonics.intec.ugent.be/contact/people.asp?ID=424"
                >Academic&nbsp;Profile</a
              >
            </li>
            <div style="padding-top: 2em;">
                <span class="fa">ï‡¹</span> Floris Laporte 2020
            </div>
          </ul>
      </div>
    </div>
    <script src="/static/js/localdates.js"></script>
    <script src="/static/js/externallinks.js"></script>
    <script src="/static/js/toggledarkmode.js"></script>
  </body>
</html>
