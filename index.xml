<rss version="2.0">
<channel>
<title>Flaport.net</title>
<link>https://blog.flaport.net/index.html</link>
<description>Flaport.net posts</description>
<generator>Flaport.net</generator>
<language>en-US</language>
<copyright>Copyright (c) 2020, all rights reserved.</copyright>
<lastBuildDate>2020-06-01T10:19:25Z</lastBuildDate>

<item>
<title> Play tic-tac-toe against a reinforcement agent. </title>
<link>https://blog.flaport.net/reinforcement-learning-part-3.html</link>
<pubDate>2020-05-30T11:27:03Z</pubDate>
<guid>https://blog.flaport.net/reinforcement-learning-part-3.html</guid>
<description>

I finally got around on making an unbeatable tic-tac-toe game. Based of course on RL part 1 and RL part 2. Have fun!

</description>
</item>

<item>
<title>Building Tiny YOLO from scratch using PyTorch</title>
<link>https://blog.flaport.net/yolo-part-1.html</link>
<pubDate>2020-05-29T12:09:34Z</pubDate>
<guid>https://blog.flaport.net/yolo-part-1.html</guid>
<description>
In this series we’ll go over YOLO (You Only Look Once), a state-of the art object detection deep neural network. In this blog post, we’ll build the simplest YOLO network: Tiny YOLO v2. This stripped down version of YOLO will yield the easiest introduction to the neural network structure of YOLO, while still providing close to state-of-the-art performance.
</description>
</item>

<item>
<title>SSH via a relay server</title>
<link>https://blog.flaport.net/ssh-via-relay-server.html</link>
<pubDate>2020-03-31T16:47:48Z</pubDate>
<guid>https://blog.flaport.net/ssh-via-relay-server.html</guid>
<description>
I often connect via SSH to my desktop computer at my desk at University. However, this computer is behind a firewall and I cannot connect to it from my laptop directly. Luckily I have my Virtual Private Server (VPS) where I - among other things - serve this blog. I use this VPS as a relay server to access my desktop from anywhere with my laptop (and vice versa).
</description>
</item>

<item>
<title>Creating a local email setup with mbsync + msmtp + neomutt + notmuch.</title>
<link>https://blog.flaport.net/configuring-neomutt-for-email.html</link>
<pubDate>2020-02-04T07:25:41Z</pubDate>
<guid>https://blog.flaport.net/configuring-neomutt-for-email.html</guid>
<description>
Neomutt is a powerful terminal email client. Using neomutt as my email client has been a real pleasure, however configuring it turns out to be a bit of a pain; there are a lot of pieces that need to fall into place. Consider this post a tutorial on how I configured Neomutt to be my email interface, while I use mbsync for syncing my email, msmtp to send email and notmuch to index my email for efficient searching.
</description>
</item>

<item>
<title>Reinforcement learning from the ground up | part 2: deep Q learning on tic-tac-toe.</title>
<link>https://blog.flaport.net/reinforcement-learning-part-2.html</link>
<pubDate>2019-12-22T14:53:34Z</pubDate>
<guid>https://blog.flaport.net/reinforcement-learning-part-2.html</guid>
<description>
Previously, we saw that reinforcement learning worked quite well on tic-tac-toe. However, there’s something unsatisfying about working with a Q-table storing all the possible states of the game. It feels like the Agent simply memorizes each state of the game and acts according to some memorized rules obtained by its huge amount of experience (remember that the Agent played 10,000,000 games during training). In this second part of the reinforcement learning series, we’ll swap out the Q table for a neural network.
</description>
</item>

<item>
<title>Reinforcement learning from the ground up | part 1: tic-tac-toe.</title>
<link>https://blog.flaport.net/reinforcement-learning-part-1.html</link>
<pubDate>2019-11-29T12:12:07Z</pubDate>
<guid>https://blog.flaport.net/reinforcement-learning-part-1.html</guid>
<description>
As a first example to reinforcement learning, we’ll make our computer learn by itself how to play tic-tac-toe. As one of the most simple 2 player games, tic-tac-toe is ideal to get started with reinforcement learning, while still being more interesting that learning to play a single player game.
</description>
</item>

</channel>
</rss>
