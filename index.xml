<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0">
<channel>
<title>Flaport.net</title>
<link>https://blog.flaport.net/index.html</link>
<description>Flaport.net posts</description>
<generator>Flaport.net</generator>
<language>en-US</language>
<copyright>Copyright (c) 2021, all rights reserved.</copyright>
<lastBuildDate>2021-03-11T12:10:00Z</lastBuildDate>

<item>
<title>A Fully connected neural network in JAX</title>
<link>https://blog.flaport.net/fully-connected-neural-network-in-jax.html</link>
<pubDate>2020-12-25T14:48:19Z</pubDate>
<guid>https://blog.flaport.net/fully-connected-neural-network-in-jax.html</guid>
<description>
JAX is the hot new ML-tool on the block. I’m currently trying to get acquinted with it. The first thing I usually do to get to know a tool like this is to make a simple neural network to solve the MNIST digit recognition task. So here we go…
</description>
</item>

<item>
<title>My notes on GPG and PGP</title>
<link>https://blog.flaport.net/notes-on-gpg.html</link>
<pubDate>2020-11-30T15:11:36Z</pubDate>
<guid>https://blog.flaport.net/notes-on-gpg.html</guid>
<description>
GPG stands for the ‘GNU Privacy Guard’. It’s the standard way to manage your PGP keys on Linux. PGP, in turn, stands for ‘Pretty Good Privacy’. With PGP you typically create a public/private key pair which are respectively used to encrypt and decrypt messages. PGP keys are mostly used to send sensitive information over the internet or to prove your identity online.
</description>
</item>

<item>
<title>Creating a Pytorch solver for sparse linear systems</title>
<link>https://blog.flaport.net/solving-sparse-linear-systems-in-pytorch.html</link>
<pubDate>2020-10-17T11:04:18Z</pubDate>
<guid>https://blog.flaport.net/solving-sparse-linear-systems-in-pytorch.html</guid>
<description>
Not so long ago, I implemented a wrapper library in PyTorch to solve sparse linear systems on the CPU using the SuiteSparse routines under the hood. My goal is to eventually integrate this sparse solver into my photonic circuit simulator, Photontorch. However, for now, I thought it would be instructive to go over the steps I took to implement both the forward pass and the backward pass of such a custom PyTorch function.
</description>
</item>

<item>
<title>Towards a loss function for YOLO</title>
<link>https://blog.flaport.net/yolo-part-2.html</link>
<pubDate>2020-09-01T09:57:11Z</pubDate>
<guid>https://blog.flaport.net/yolo-part-2.html</guid>
<description>
Writing the second part of the YOLO series took a lot longer than I care to admit… but, it’s finally here! In this part we’ll go over the definition of a loss function to train the YOLO architecture.
</description>
</item>

<item>
<title>Wrapping your head around the most used bash operators</title>
<link>https://blog.flaport.net/bash-operators.html</link>
<pubDate>2020-07-26T15:06:55Z</pubDate>
<guid>https://blog.flaport.net/bash-operators.html</guid>
<description>
This is mostly geared towards helping bash-beginners. Having a sound understanding of what each bash operator does might help you becoming a bash-wizard ;)
</description>
</item>

<item>
<title> Play tic-tac-toe against a reinforcement agent </title>
<link>https://blog.flaport.net/reinforcement-learning-part-3.html</link>
<pubDate>2020-05-30T11:27:03Z</pubDate>
<guid>https://blog.flaport.net/reinforcement-learning-part-3.html</guid>
<description>
I finally made an unbeatable tic-tac-toe game in javascript using the qtable obtained in in the first reinforcement learning post. Have Fun going for the tie!
</description>
</item>

<item>
<title>Building Tiny YOLO from scratch using PyTorch</title>
<link>https://blog.flaport.net/yolo-part-1.html</link>
<pubDate>2020-04-23T12:09:34Z</pubDate>
<guid>https://blog.flaport.net/yolo-part-1.html</guid>
<description>
In this series we’ll go over YOLO (You Only Look Once), a state-of the art object detection deep neural network. In this blog post, we’ll build the simplest YOLO network: Tiny YOLO v2. This stripped down version of YOLO will yield the easiest introduction to the neural network structure of YOLO, while still providing close to state-of-the-art performance.
</description>
</item>

<item>
<title>SSH via a relay server</title>
<link>https://blog.flaport.net/ssh-via-relay-server.html</link>
<pubDate>2020-03-31T16:47:48Z</pubDate>
<guid>https://blog.flaport.net/ssh-via-relay-server.html</guid>
<description>
I often connect via SSH to my desktop computer at my desk at University. However, this computer is behind a firewall and I cannot connect to it from my laptop directly. Luckily I have my Virtual Private Server (VPS) where I - among other things - serve this blog. I use this VPS as a relay server to access my desktop from anywhere with my laptop (and vice versa).
</description>
</item>

<item>
<title>Creating a local email setup with mbsync + msmtp + neomutt + notmuch.</title>
<link>https://blog.flaport.net/configuring-neomutt-for-email.html</link>
<pubDate>2020-02-04T07:25:41Z</pubDate>
<guid>https://blog.flaport.net/configuring-neomutt-for-email.html</guid>
<description>
Neomutt is a powerful terminal email client. Using neomutt as my email client has been a real pleasure, however configuring it turns out to be a bit of a pain; there are a lot of pieces that need to fall into place. Consider this post a tutorial on how I configured Neomutt to be my email interface, while I use mbsync for syncing my email, msmtp to send email and notmuch to index my email for efficient searching.
</description>
</item>

<item>
<title>Reinforcement learning from the ground up | part 2: deep Q learning on tic-tac-toe.</title>
<link>https://blog.flaport.net/reinforcement-learning-part-2.html</link>
<pubDate>2019-12-22T14:53:34Z</pubDate>
<guid>https://blog.flaport.net/reinforcement-learning-part-2.html</guid>
<description>
Previously, we saw that reinforcement learning worked quite well on tic-tac-toe. However, there’s something unsatisfying about working with a Q-table storing all the possible states of the game. It feels like the Agent simply memorizes each state of the game and acts according to some memorized rules obtained by its huge amount of experience (remember that the Agent played 10,000,000 games during training). In this second part of the reinforcement learning series, we’ll swap out the Q table for a neural network.
</description>
</item>

<item>
<title>Reinforcement learning from the ground up | part 1: tic-tac-toe.</title>
<link>https://blog.flaport.net/reinforcement-learning-part-1.html</link>
<pubDate>2019-11-29T12:12:07Z</pubDate>
<guid>https://blog.flaport.net/reinforcement-learning-part-1.html</guid>
<description>
As a first example to reinforcement learning, we’ll make our computer learn by itself how to play tic-tac-toe. As one of the most simple 2 player games, tic-tac-toe is ideal to get started with reinforcement learning, while still being more interesting that learning to play a single player game.
</description>
</item>

</channel>
</rss>
